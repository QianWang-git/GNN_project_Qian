{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNNStack notes\n",
    "**CS224W stanford**  \n",
    "**https://colab.research.google.com/drive/1DIQm9rOx2mT1bZETEeVUThxcrP1RKqAn#scrollTo=XyzIhe0O5ije**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot\n",
    "from torchviz import make_dot\n",
    "from torch.autograd import Variable\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.datasets import CitationFull\n",
    "from tqdm import tqdm\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/citeseer.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/dblp.npz\n",
      "Processing...\n",
      "Done!\n",
      "Downloading https://github.com/abojchevski/graph2gauss/raw/master/data/pubmed.npz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "data_cora = CitationFull('./CitationFull','cora')\n",
    "data_cora_ml = CitationFull('./CitationFull','cora_ml')\n",
    "data_citeseer =  CitationFull('./CitationFull','citeseer')\n",
    "data_dblp =  CitationFull('./CitationFull','dblp')\n",
    "data_pubmed =  CitationFull('./CitationFull','pubmed')\n",
    "#data_raw = read_npz('./CitationFull/cora/raw/cora.npz')\n",
    "#print(data_raw) # numer of edge_index: 126842 \n",
    "#print(data.num_node_features) # Returns the number of features per node in the graph.\n",
    "#print(data.num_classes) # Returns the number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset | num of edges | num of nodes | size of node features | num of num_classes\n",
    "- | :-: | :-: | :-: | -:\n",
    "cora | 126842 | 19793 | 8710 | 70| \n",
    "cora_ml | 16316| 2995 | 2879 | 7| \n",
    "citeseer | 10674|4230 | 602 | 6|\n",
    "dblp | 105734| 17716| 1639| 4|\n",
    "pubmed | 88648| 19717| 500| 3|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNStack(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, task='node'):\n",
    "        super(GNNStack, self).__init__()\n",
    "        self.task = task\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(self.build_conv_model(input_dim, hidden_dim))\n",
    "        self.lns = nn.ModuleList()\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        self.lns.append(nn.LayerNorm(hidden_dim))\n",
    "        for l in range(2):\n",
    "            self.convs.append(self.build_conv_model(hidden_dim, hidden_dim))\n",
    "\n",
    "        # post-message-passing\n",
    "        self.post_mp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim), nn.Dropout(0.25), \n",
    "            nn.Linear(hidden_dim, output_dim))\n",
    "        if not (self.task == 'node' or self.task == 'graph'):\n",
    "            raise RuntimeError('Unknown task.')\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.num_layers = 3\n",
    "\n",
    "    def build_conv_model(self, input_dim, hidden_dim):\n",
    "        # refer to pytorch geometric nn module for different implementation of GNNs.\n",
    "        if self.task == 'node':\n",
    "            return pyg_nn.GCNConv(input_dim, hidden_dim)    # symmetric normalized Laplacian\n",
    "        else:\n",
    "            return pyg_nn.GINConv(nn.Sequential(nn.Linear(input_dim, hidden_dim),\n",
    "                                  nn.ReLU(), nn.Linear(hidden_dim, hidden_dim)))\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        if data.num_node_features == 0:\n",
    "            x = torch.ones(data.num_nodes, 1)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            emb = x\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            if not i == self.num_layers - 1:\n",
    "                x = self.lns[i](x)\n",
    "\n",
    "        if self.task == 'graph':\n",
    "            x = pyg_nn.global_mean_pool(x, batch)\n",
    "\n",
    "        x = self.post_mp(x)\n",
    "\n",
    "        return emb, F.log_softmax(x, dim=1)\n",
    "\n",
    "    def loss(self, pred, label):\n",
    "        return F.nll_loss(pred, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, task, writer):\n",
    "    if task == 'graph':\n",
    "        data_size = len(dataset)\n",
    "        loader = DataLoader(dataset[:int(data_size * 0.8)], batch_size=64, shuffle=True)\n",
    "        test_loader = DataLoader(dataset[int(data_size * 0.8):], batch_size=64, shuffle=True)\n",
    "    else:\n",
    "        test_loader = loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    # build model\n",
    "    model = GNNStack(max(dataset.num_node_features, 1), 32, dataset.num_classes, task=task)\n",
    "    opt = optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # train\n",
    "    for epoch in range(150):\n",
    "        total_loss = 0\n",
    "        model.train()\n",
    "        for batch in loader:\n",
    "            #print(batch.train_mask, '----')\n",
    "            opt.zero_grad()\n",
    "            embedding, pred = model(batch)\n",
    "            #print(embedding.shape)     # why not batch size？？？？？？？？？？？？？？？？？？？ not 64\n",
    "            label = batch.y\n",
    "            loss = model.loss(pred, label)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item() * batch.num_graphs\n",
    "        total_loss /= len(loader.dataset)\n",
    "        writer.add_scalar(\"loss\", total_loss, epoch)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            test_acc = test(test_loader, model)\n",
    "            print(\"Epoch {}. Loss: {:.4f}. Test accuracy: {:.4f}\".format(\n",
    "                epoch, total_loss, test_acc))\n",
    "            writer.add_scalar(\"test accuracy\", test_acc, epoch)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model, is_validation=False):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        with torch.no_grad():\n",
    "            emb, pred = model(data)\n",
    "            pred = pred.argmax(dim=1)\n",
    "            label = data.y\n",
    "\n",
    "        #if model.task == 'node':\n",
    "            #mask = data.val_mask if is_validation else data.test_mask\n",
    "            # node classification: only evaluate on nodes in test set\n",
    "            #pred = pred[mask]\n",
    "            #label = data.y[mask]\n",
    "            \n",
    "        correct += pred.eq(label).sum().item()\n",
    "    \n",
    "    if model.task == 'graph':\n",
    "        total = len(loader.dataset) \n",
    "    else:\n",
    "        total = 0\n",
    "        for data in loader.dataset:\n",
    "            #print(len(data.y))\n",
    "            #total += torch.sum(data.test_mask).item()\n",
    "            total += len(data.y)\n",
    "    #print(total)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataset =[data_cora,data_cora_ml,data_citeseer,data_dblp,data_pubmed]\n",
    "all_dataset_name = ['cora','cora_ml','citeseer','dblp','pubmed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 4.2978. Test accuracy: 0.0425\n",
      "Epoch 10. Loss: 3.2422. Test accuracy: 0.2871\n",
      "Epoch 20. Loss: 2.3083. Test accuracy: 0.5702\n",
      "Epoch 30. Loss: 1.8494. Test accuracy: 0.6709\n",
      "Epoch 40. Loss: 1.5955. Test accuracy: 0.7316\n",
      "Epoch 50. Loss: 1.4260. Test accuracy: 0.7655\n",
      "Epoch 60. Loss: 1.3177. Test accuracy: 0.7837\n",
      "Epoch 70. Loss: 1.2298. Test accuracy: 0.8043\n",
      "Epoch 80. Loss: 1.1664. Test accuracy: 0.8149\n",
      "Epoch 90. Loss: 1.1224. Test accuracy: 0.8220\n",
      "Epoch 100. Loss: 1.0808. Test accuracy: 0.8297\n",
      "Epoch 110. Loss: 1.0557. Test accuracy: 0.8344\n",
      "Epoch 120. Loss: 1.0090. Test accuracy: 0.8396\n",
      "Epoch 130. Loss: 0.9907. Test accuracy: 0.8440\n",
      "Epoch 140. Loss: 0.9641. Test accuracy: 0.8480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "1it [03:50, 230.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.9674. Test accuracy: 0.2928\n",
      "Epoch 10. Loss: 0.6544. Test accuracy: 0.8965\n",
      "Epoch 20. Loss: 0.3723. Test accuracy: 0.9312\n",
      "Epoch 30. Loss: 0.2876. Test accuracy: 0.9392\n",
      "Epoch 40. Loss: 0.2422. Test accuracy: 0.9476\n",
      "Epoch 50. Loss: 0.1932. Test accuracy: 0.9543\n",
      "Epoch 60. Loss: 0.1827. Test accuracy: 0.9636\n",
      "Epoch 70. Loss: 0.1684. Test accuracy: 0.9646\n",
      "Epoch 80. Loss: 0.1626. Test accuracy: 0.9656\n",
      "Epoch 90. Loss: 0.1509. Test accuracy: 0.9710\n",
      "Epoch 100. Loss: 0.1338. Test accuracy: 0.9703\n",
      "Epoch 110. Loss: 0.1247. Test accuracy: 0.9756\n",
      "Epoch 120. Loss: 0.1437. Test accuracy: 0.9740\n",
      "Epoch 130. Loss: 0.1214. Test accuracy: 0.9780\n",
      "Epoch 140. Loss: 0.1110. Test accuracy: 0.9790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [04:06, 166.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.8187. Test accuracy: 0.5173\n",
      "Epoch 10. Loss: 0.3267. Test accuracy: 0.9608\n",
      "Epoch 20. Loss: 0.1524. Test accuracy: 0.9818\n",
      "Epoch 30. Loss: 0.1090. Test accuracy: 0.9853\n",
      "Epoch 40. Loss: 0.0729. Test accuracy: 0.9851\n",
      "Epoch 50. Loss: 0.0581. Test accuracy: 0.9884\n",
      "Epoch 60. Loss: 0.0612. Test accuracy: 0.9917\n",
      "Epoch 70. Loss: 0.0525. Test accuracy: 0.9929\n",
      "Epoch 80. Loss: 0.0502. Test accuracy: 0.9920\n",
      "Epoch 90. Loss: 0.0490. Test accuracy: 0.9924\n",
      "Epoch 100. Loss: 0.0419. Test accuracy: 0.9943\n",
      "Epoch 110. Loss: 0.0324. Test accuracy: 0.9943\n",
      "Epoch 120. Loss: 0.0373. Test accuracy: 0.9943\n",
      "Epoch 130. Loss: 0.0483. Test accuracy: 0.9953\n",
      "Epoch 140. Loss: 0.0317. Test accuracy: 0.9960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [04:15, 118.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.4054. Test accuracy: 0.4507\n",
      "Epoch 10. Loss: 0.4759. Test accuracy: 0.8586\n",
      "Epoch 20. Loss: 0.3766. Test accuracy: 0.8810\n",
      "Epoch 30. Loss: 0.3235. Test accuracy: 0.8908\n",
      "Epoch 40. Loss: 0.2899. Test accuracy: 0.9023\n",
      "Epoch 50. Loss: 0.2599. Test accuracy: 0.9103\n",
      "Epoch 60. Loss: 0.2352. Test accuracy: 0.9174\n",
      "Epoch 70. Loss: 0.2209. Test accuracy: 0.9240\n",
      "Epoch 80. Loss: 0.2023. Test accuracy: 0.9274\n",
      "Epoch 90. Loss: 0.1959. Test accuracy: 0.9322\n",
      "Epoch 100. Loss: 0.1778. Test accuracy: 0.9366\n",
      "Epoch 110. Loss: 0.1711. Test accuracy: 0.9358\n",
      "Epoch 120. Loss: 0.1675. Test accuracy: 0.9410\n",
      "Epoch 130. Loss: 0.1574. Test accuracy: 0.9434\n",
      "Epoch 140. Loss: 0.1537. Test accuracy: 0.9434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [05:37, 107.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 1.1047. Test accuracy: 0.4112\n",
      "Epoch 10. Loss: 0.5438. Test accuracy: 0.8352\n",
      "Epoch 20. Loss: 0.4184. Test accuracy: 0.8695\n",
      "Epoch 30. Loss: 0.3630. Test accuracy: 0.8856\n",
      "Epoch 40. Loss: 0.3356. Test accuracy: 0.8946\n",
      "Epoch 50. Loss: 0.3030. Test accuracy: 0.9005\n",
      "Epoch 60. Loss: 0.2886. Test accuracy: 0.9064\n",
      "Epoch 70. Loss: 0.2640. Test accuracy: 0.9137\n",
      "Epoch 80. Loss: 0.2495. Test accuracy: 0.9179\n",
      "Epoch 90. Loss: 0.2404. Test accuracy: 0.9210\n",
      "Epoch 100. Loss: 0.2294. Test accuracy: 0.9261\n",
      "Epoch 110. Loss: 0.2192. Test accuracy: 0.9299\n",
      "Epoch 120. Loss: 0.2184. Test accuracy: 0.9300\n",
      "Epoch 130. Loss: 0.2165. Test accuracy: 0.9321\n",
      "Epoch 140. Loss: 0.2097. Test accuracy: 0.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [06:38, 79.70s/it] \n"
     ]
    }
   ],
   "source": [
    "for i,j in tqdm(zip(all_dataset,all_dataset_name)):\n",
    "    writer = SummaryWriter(\"./log_\"+j+'/' + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    dataset = i.shuffle()\n",
    "    task = 'node'\n",
    "    model = train(dataset, task, writer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model structure\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "plot_model = GNNStack(max(dataset.num_node_features, 1), 32, dataset.num_classes, task=task)\n",
    "for batch in loader:\n",
    "    y = plot_model(batch)\n",
    "    g = make_dot(y, params=dict(plot_model.named_parameters()))\n",
    "    g.view()\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
